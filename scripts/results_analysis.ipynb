{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openml\n",
    "from scipy.stats import ttest_rel, rankdata\n",
    "import lightgbm as lgb\n",
    "from src import DataBinner\n",
    "import numpy as np\n",
    "\n",
    "num_datasets_class = 16\n",
    "num_datasets_reg = 18\n",
    "\n",
    "num_seeds = 20\n",
    "model = 'SKL'\n",
    "\n",
    "reg_benchmark_suite = openml.study.get_suite(336)\n",
    "class_benchmark_suite = openml.study.get_suite(337)  # 337 for classification\n",
    "\n",
    "#We want to be ranking across datasets within each model type\n",
    "mrr_reg = {'quantile': [], 'linspace': [], 'kmeans': []}\n",
    "mrr_class = {'quantile': [], 'linspace': [], 'kmeans': []}\n",
    "mrr_total = {'quantile': [], 'linspace': [], 'kmeans': []}\n",
    "    \n",
    "def bold_if_sig(best_val, best_arr, second_val, second_arr):\n",
    "    \"\"\"Return '{best_val:.5f}' or '\\\\textbf{...}' depending on p-value.\"\"\"\n",
    "    p_val = ttest_rel(best_arr, second_arr).pvalue\n",
    "    formatted = f\"${best_val:.3f}$\"\n",
    "\n",
    "    if p_val < 0.05:\n",
    "        return f\"${formatted}\\\\mathrlap{{^{{**}}}}$\"\n",
    "    elif p_val < 0.1:\n",
    "        return f\"${formatted}\\\\mathrlap{{^{{*}}}}$\"\n",
    "    else:\n",
    "        return formatted\n",
    "\n",
    "#Classification first\n",
    "print(\"DATA FOR CLASSIFICATION\")\n",
    "for idx in range(num_datasets_class):\n",
    "    if idx not in [2, 8]:\n",
    "        task_id = class_benchmark_suite.tasks[idx]\n",
    "        task = openml.tasks.get_task(task_id)\n",
    "        dataset = task.get_dataset()\n",
    "        name = dataset.name\n",
    "        name = name.replace(\"_\", \"\\_\")\n",
    "        obs = dataset.qualities['NumberOfInstances']\n",
    "        features = dataset.qualities['NumberOfFeatures']\n",
    "\n",
    "        with open(f\"class_results_binning_{idx}.json\", \"r\") as f:\n",
    "            results = json.load(f)\n",
    "            linspace_dict = results['linspace']\n",
    "            quantile_dict = results['quantile'] \n",
    "            kmeans_dict = results['kmeans']\n",
    "            exact_dict = results['exact']\n",
    "\n",
    "        kmeans_results = kmeans_dict[model]['roc_auc']\n",
    "        linspace_results = linspace_dict[model]['roc_auc']\n",
    "        quantile_results = quantile_dict[model]['roc_auc']\n",
    "        exact_results = exact_dict[model]['roc_auc']\n",
    "\n",
    "        kmeans_mean = np.mean(kmeans_results)\n",
    "        linspace_mean = np.mean(linspace_results)\n",
    "        quantile_mean = np.mean(quantile_results)\n",
    "        exact_mean = np.mean(exact_results)\n",
    "        \n",
    "        # Calculating MRR within this dataset -- DO NOT include exact here\n",
    "        results = {'kmeans': kmeans_results, 'linspace': linspace_results, 'quantile': quantile_results}\n",
    "        means = {method: np.mean(results[method]) for method in results}\n",
    "        rank_list = list(means.values())\n",
    "        ranks = {method: rank for method, rank in zip(means.keys(), rankdata(-1 * np.array(rank_list)))}\n",
    "        \n",
    "        # Get inverse ranks\n",
    "        inv_ranks = {method: 1 / rank for method, rank in ranks.items()}\n",
    "        \n",
    "        for method in results.keys():\n",
    "            mrr_class[method].append(inv_ranks[method])\n",
    "            mrr_total[method].append(inv_ranks[method])\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Find best / second-best (lower MSE = better) and format cells\n",
    "        # ------------------------------------------------------------\n",
    "        sorted_methods = sorted(means, key=means.get, reverse = True)          # best first\n",
    "        best, second = sorted_methods[:2]\n",
    "\n",
    "        fmt = {}                                              # holds strings to print\n",
    "        for m in ['quantile', 'linspace', 'kmeans']:\n",
    "            if m == best:                                     # winner – maybe bold\n",
    "                fmt[m] = bold_if_sig(means[m],\n",
    "                                    results[m],              # arrays for t-test\n",
    "                                    means[second],\n",
    "                                    results[second])\n",
    "            else:                                             # plain number\n",
    "                fmt[m] = f\"{(means[m]):.3f}\"\n",
    "                \n",
    "        print(f\"{name} & {fmt['quantile']} & {fmt['linspace']} & {fmt['kmeans']} & {exact_mean:.3f} \\\\\\\\\")\n",
    "\n",
    "    \n",
    "#Dictionary that we put our final rankings in\n",
    "mrr_class_avg = {method: np.mean(mrr_class[method]) for method in ['quantile', 'linspace', 'kmeans']}\n",
    "        \n",
    "\n",
    "# MRR for Classification\n",
    "print(\"%% Mean Reciprocal Rank (MRR) Table for Classification\")\n",
    "print(\"\\\\begin{table}[htbp]\")\n",
    "print(\"    \\\\centering\")\n",
    "print(\"    \\\\caption{Mean Reciprocal Rank (MRR) for each binning method computed within each baseline algorithm based on MSE performance for Regression datasets. Lower MSE yields a better (lower) rank, and the inverse rank is averaged across datasets.}\")\n",
    "print(\"    \\\\label{tab:mrr_class}\")\n",
    "print(\"    \\\\begin{tabular}{lccc}\")\n",
    "print(\"        \\\\toprule\")\n",
    "print(\"        Baseline & Quantile & Uniform & K-Means \\\\\\\\\")\n",
    "print(\"        \\\\midrule\")\n",
    "print(f\"        \\\\textbf{{Classification}} & {mrr_class_avg['quantile']:.2f} & {mrr_class_avg['linspace']:.2f} & {mrr_class_avg['kmeans']:.2f} & \\\\\\\\\")\n",
    "print(\"        \\\\bottomrule\")\n",
    "print(\"    \\\\end{tabular}\")\n",
    "print(\"\\\\end{table}\")\n",
    "\n",
    "\n",
    "#Regression\n",
    "print(\"DATA FOR REGRESSION\")\n",
    "for idx in range(num_datasets_reg):\n",
    "    task_id = reg_benchmark_suite.tasks[idx]\n",
    "    task = openml.tasks.get_task(task_id)\n",
    "    dataset = task.get_dataset()\n",
    "    name = dataset.name\n",
    "    name = name.replace(\"_\", \"\\_\")\n",
    "    obs = dataset.qualities['NumberOfInstances']\n",
    "    features = dataset.qualities['NumberOfFeatures']\n",
    "\n",
    "    with open(f\"reg_results_binning_{idx}.json\", \"r\") as f:\n",
    "        results = json.load(f)\n",
    "        linspace_dict = results['linspace']\n",
    "        quantile_dict = results['quantile'] \n",
    "        kmeans_dict = results['kmeans']\n",
    "        exact_dict = results['exact']\n",
    "\n",
    "    kmeans_results = kmeans_dict[model]['mse']\n",
    "    linspace_results = linspace_dict[model]['mse']\n",
    "    quantile_results = quantile_dict[model]['mse']\n",
    "    exact_results = exact_dict[model]['mse']\n",
    "\n",
    "    kmeans_mean = np.mean(kmeans_results)\n",
    "    #Find correct value for exponent for scientific notation\n",
    "    exponent = int(np.floor(np.log10(kmeans_mean)))\n",
    "    divisor = 10 ** exponent\n",
    "    \n",
    "    linspace_mean = np.mean(linspace_results)\n",
    "    quantile_mean = np.mean(quantile_results)\n",
    "    exact_mean = np.mean(exact_results)\n",
    "    \n",
    "    # Calculating MRR within this dataset -- DO NOT include exact here\n",
    "    results = {'kmeans': kmeans_results, 'linspace': linspace_results, 'quantile': quantile_results}\n",
    "    means = {method: np.mean(results[method]) for method in results}\n",
    "    ranks = {method: rank for method, rank in zip(means.keys(), rankdata(list(means.values())))}\n",
    "    \n",
    "    # Get inverse ranks\n",
    "    inv_ranks = {method: 1 / rank for method, rank in ranks.items()}\n",
    "    \n",
    "    for method in results.keys():\n",
    "        mrr_reg[method].append(inv_ranks[method])\n",
    "        mrr_total[method].append(inv_ranks[method])\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Find best / second-best (lower MSE = better) and format cells\n",
    "    # ------------------------------------------------------------\n",
    "    sorted_methods = sorted(means, key=means.get)          # best first\n",
    "    best, second = sorted_methods[:2]\n",
    "\n",
    "    fmt = {}                                              # holds strings to print\n",
    "    for m in ['quantile', 'linspace', 'kmeans']:\n",
    "        if m == best:                                     # winner – maybe bold\n",
    "            fmt[m] = bold_if_sig(means[m] / divisor,\n",
    "                                  results[m],              # arrays for t-test\n",
    "                                  means[second] / divisor,\n",
    "                                  results[second])\n",
    "        else:                                             # plain number\n",
    "            fmt[m] = f\"{(means[m] / divisor):.3f}\"\n",
    "            \n",
    "    print(f\"{name} $(10^{{{exponent}}})$ & {fmt['quantile']} & {fmt['linspace']} & {fmt['kmeans']} & {(exact_mean / divisor) :.3f} \\\\\\\\\")\n",
    "    \n",
    "#Dictionary that we put our final rankings in\n",
    "mrr_reg_avg = {method: np.mean(mrr_reg[method]) for method in ['quantile', 'linspace', 'kmeans']}\n",
    "\n",
    "\n",
    "# MRR for Regression\n",
    "print(\"%% Mean Reciprocal Rank (MRR) Table for Regression\")\n",
    "print(\"\\\\begin{table}[htbp]\")\n",
    "print(\"    \\\\centering\")\n",
    "print(\"    \\\\caption{Mean Reciprocal Rank (MRR) for each binning method computed within each baseline algorithm based on MSE performance for Regression datasets. Lower MSE yields a better (lower) rank, and the inverse rank is averaged across datasets.}\")\n",
    "print(\"    \\\\label{tab:mrr_reg}\")\n",
    "print(\"    \\\\begin{tabular}{lccc}\")\n",
    "print(\"        \\\\toprule\")\n",
    "print(\"        Baseline & Quantile & Uniform & K-Means \\\\\\\\\")\n",
    "print(\"        \\\\midrule\")\n",
    "print(f\"        \\\\textbf{{Regression}} & {mrr_reg_avg['quantile']:.2f} & {mrr_reg_avg['linspace']:.2f} & {mrr_reg_avg['kmeans']:.2f} & \\\\\\\\\")\n",
    "print(\"        \\\\bottomrule\")\n",
    "print(\"    \\\\end{tabular}\")\n",
    "print(\"\\\\end{table}\")\n",
    "\n",
    "#TOTAL!\n",
    "#Dictionary that we put our final rankings in\n",
    "mrr_total_avg = {method: np.mean(mrr_total[method]) for method in ['quantile', 'linspace', 'kmeans']}\n",
    "print(\"TOTAL DATA\")\n",
    "# MRR for Regression\n",
    "print(\"%% Mean Reciprocal Rank (MRR) Table for All\")\n",
    "print(\"\\\\begin{table}[htbp]\")\n",
    "print(\"    \\\\centering\")\n",
    "print(\"    \\\\caption{Mean Reciprocal Rank (MRR) for each binning method over all datasets}\")\n",
    "print(\"    \\\\label{tab:mrr_reg}\")\n",
    "print(\"    \\\\begin{tabular}{lccc}\")\n",
    "print(\"        \\\\toprule\")\n",
    "print(\"        Baseline & Quantile & Uniform & K-Means \\\\\\\\\")\n",
    "print(\"        \\\\midrule\")\n",
    "print(f\"        \\\\textbf{{Total}} & {mrr_total_avg['quantile']:.2f} & {mrr_total_avg['linspace']:.2f} & {mrr_total_avg['kmeans']:.2f} & \\\\\\\\\")\n",
    "print(\"        \\\\bottomrule\")\n",
    "print(\"    \\\\end{tabular}\")\n",
    "print(\"\\\\end{table}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openml\n",
    "from scipy.stats import ttest_rel, rankdata\n",
    "import lightgbm as lgb\n",
    "from src import DataBinner\n",
    "import numpy as np\n",
    "\n",
    "num_datasets_class = 16\n",
    "num_datasets_reg = 18\n",
    "\n",
    "num_seeds = 20\n",
    "model = 'SKL'\n",
    "\n",
    "reg_benchmark_suite = openml.study.get_suite(336)\n",
    "class_benchmark_suite = openml.study.get_suite(337)  # 337 for classification\n",
    "\n",
    "print(\"Classification\")\n",
    "class_frac = []\n",
    "for idx in range(num_datasets_class):\n",
    "    task_id = class_benchmark_suite.tasks[idx]\n",
    "    task = openml.tasks.get_task(task_id)\n",
    "    dataset = task.get_dataset()\n",
    "    name = dataset.name\n",
    "\n",
    "    X, y = task.get_X_and_y(dataset_format='dataframe')\n",
    "    cols_with_255_unique = 0\n",
    "    skews = []\n",
    "    for col in X.columns:\n",
    "        if len(X[col].unique()) >= 255:\n",
    "            skews.append(X[col].skew())\n",
    "            cols_with_255_unique += 1\n",
    "    class_frac.append(cols_with_255_unique / len(X.columns))\n",
    "    print(f\"Dataset: {name}, fraction of features with 255 unique values: {cols_with_255_unique / len(X.columns):.2f}, skew: {np.mean(skews):.2f}\")\n",
    "    \n",
    "print(\"Regression\")\n",
    "reg_frac = []\n",
    "for idx in range(num_datasets_reg):\n",
    "    task_id = reg_benchmark_suite.tasks[idx]\n",
    "    task = openml.tasks.get_task(task_id)\n",
    "    dataset = task.get_dataset()\n",
    "    name = dataset.name\n",
    "    \n",
    "    X, y = task.get_X_and_y(dataset_format='dataframe')\n",
    "    cols_with_255_unique = 0\n",
    "    skews = []\n",
    "    for col in X.columns:\n",
    "        if len(X[col].unique()) >= 255:\n",
    "            skews.append(X[col].skew())\n",
    "            cols_with_255_unique += 1\n",
    "    reg_frac.append(cols_with_255_unique / len(X.columns))\n",
    "    print(f\"Dataset: {name}, fraction of features with 255 unique values: {cols_with_255_unique / len(X.columns):.2f}, skew: {np.mean(skews):.2f}\")\n",
    "    \n",
    "print(np.mean(reg_frac), np.mean(class_frac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name:  pol\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "bins must be monotonically increasing or decreasing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m quantile_binner = DataBinner(method=\u001b[33m'\u001b[39m\u001b[33mquantile\u001b[39m\u001b[33m'\u001b[39m, n_bins=\u001b[32m255\u001b[39m)\n\u001b[32m     15\u001b[39m kmeans_binner = DataBinner(method=\u001b[33m'\u001b[39m\u001b[33mkmeans\u001b[39m\u001b[33m'\u001b[39m, n_bins=\u001b[32m255\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m X_q, X_k = \u001b[43mquantile_binner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, kmeans_binner.fit_transform(X)\n\u001b[32m     18\u001b[39m splits_q, splits_k = quantile_binner._models, kmeans_binner._models\n\u001b[32m     20\u001b[39m X_0 = X.iloc[:, idx]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/HistBinningLGBM/src/bin_dataset.py:114\u001b[39m, in \u001b[36mDataBinner.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/HistBinningLGBM/src/bin_dataset.py:106\u001b[39m, in \u001b[36mDataBinner.transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    103\u001b[39m out = np.zeros_like(X_arr, dtype=\u001b[38;5;28mint\u001b[39m)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, breakpoints \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m._models):\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     out[:, j] = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdigitize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_arr\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbreakpoints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cols \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    109\u001b[39m     out = pd.DataFrame(out, columns=cols)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/HistBinningLGBM/HistMethods/lib/python3.11/site-packages/numpy/lib/function_base.py:5725\u001b[39m, in \u001b[36mdigitize\u001b[39m\u001b[34m(x, bins, right)\u001b[39m\n\u001b[32m   5723\u001b[39m mono = _monotonicity(bins)\n\u001b[32m   5724\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mono == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m5725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mbins must be monotonically increasing or decreasing\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5727\u001b[39m \u001b[38;5;66;03m# this is backwards because the arguments below are swapped\u001b[39;00m\n\u001b[32m   5728\u001b[39m side = \u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m right \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mright\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: bins must be monotonically increasing or decreasing"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reg_benchmark_suite = openml.study.get_suite(336)\n",
    "class_benchmark_suite = openml.study.get_suite(337)  # 337 for classification\n",
    "\n",
    "dataset_idx = 0\n",
    "col_idx = 0\n",
    "task_id = class_benchmark_suite.tasks[idx]\n",
    "task = openml.tasks.get_task(task_id)\n",
    "dataset = task.get_dataset()\n",
    "name = dataset.name\n",
    "print(\"Dataset name: \", name)\n",
    "\n",
    "X, y = task.get_X_and_y(dataset_format='dataframe')\n",
    "quantile_binner = DataBinner(method='quantile', n_bins=255)\n",
    "kmeans_binner = DataBinner(method='kmeans', n_bins=255)\n",
    "\n",
    "X_q, X_k = quantile_binner.fit_transform(X), kmeans_binner.fit_transform(X)\n",
    "splits_q, splits_k = quantile_binner._models, kmeans_binner._models\n",
    "\n",
    "X_0 = X.iloc[:, idx]\n",
    "splits_q = splits_q[idx]\n",
    "splits_k = splits_k[idx]\n",
    "\n",
    "#Create graph comparing the two binning methods\n",
    "plt.hist(X_0, bins = 100, alpha = 0.5, label = \"Original\", color = \"blue\")\n",
    "plt.axvline(x = splits_q[0], color = \"red\", linestyle = \"--\", label = f\"Quantile bin\")\n",
    "plt.axvline(x = splits_k[0], color = \"green\", linestyle = \"--\", label = f\"K-Means bin\")\n",
    "for i in range(1, 254):\n",
    "    plt.axvline(x = splits_q[i], color = \"red\", linestyle = \"--\")\n",
    "    plt.axvline(x = splits_k[i], color = \"green\", linestyle = \"--\")\n",
    "plt.legend()\n",
    "plt.title(f\"Comparison of Quantile and K-Means Binning for {name}\")\n",
    "plt.yscale('log')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HistMethods",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
